{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU-pLW33lpcS"
      },
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loi8Sh7UE6ha"
      },
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "outputId": "1477b349-0031-44c6-fb14-d9534a982236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training and Cross-Validating MultinomialNB...\n",
            "MultinomialNB Cross-Validation Accuracy: 0.7747527434864638\n",
            "MultinomialNB Evaluation Metrics:\n",
            "Accuracy: 0.7969653179190751, Precision: 0.7571428571428571, Recall: 0.8920056100981767, F1-score: 0.8190598840952994\n",
            "Training and Cross-Validating SVM...\n",
            "SVM Cross-Validation Accuracy: 0.7724042146219178\n",
            "SVM Evaluation Metrics:\n",
            "Accuracy: 0.7976878612716763, Precision: 0.7730138713745272, Recall: 0.8597475455820477, F1-score: 0.8140770252324037\n",
            "Training and Cross-Validating KNN...\n",
            "KNN Cross-Validation Accuracy: 0.71242876074709\n",
            "KNN Evaluation Metrics:\n",
            "Accuracy: 0.7276011560693642, Precision: 0.7131979695431472, Recall: 0.788218793828892, F1-score: 0.7488341105929379\n",
            "Training and Cross-Validating Decision Tree...\n",
            "Decision Tree Cross-Validation Accuracy: 0.6083815877948309\n",
            "Decision Tree Evaluation Metrics:\n",
            "Accuracy: 0.6083815028901735, Precision: 0.6135458167330677, Recall: 0.6479663394109397, F1-score: 0.6302864938608458\n",
            "Training and Cross-Validating Random Forest...\n",
            "Random Forest Cross-Validation Accuracy: 0.7077313766067592\n",
            "Random Forest Evaluation Metrics:\n",
            "Accuracy: 0.7189306358381503, Precision: 0.6980440097799511, Recall: 0.8008415147265077, F1-score: 0.7459177008491182\n",
            "Final Evaluation Metrics on Test Data:\n",
            "Accuracy: 0.8023064250411862, Precision: 0.7567820392890552, Recall: 0.88998899889989, F1-score: 0.8179979777553085\n"
          ]
        }
      ],
      "source": [
        "#import the necessary libarries.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load Data\n",
        "def load_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "    data = [line.strip().split(' ', 1) for line in lines] # Split each line at the first space\n",
        "    df = pd.DataFrame(data, columns=['label', 'text'])\n",
        "    return df\n",
        "\n",
        "train_data = load_data('stsa-train.txt')\n",
        "test_data = load_data('stsa-test.txt')\n",
        "\n",
        "# Data Preprocessing and Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['text'])\n",
        "y_train = train_data['label']\n",
        "\n",
        "# Split Data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_tfidf, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Cross-Validation and Training\n",
        "models = {\n",
        "    'MultinomialNB': MultinomialNB(),\n",
        "    'SVM': SVC(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    #'XGBoost': XGBClassifier(),\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Training and Cross-Validating {name}...\")\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy')\n",
        "    print(f\"{name} Cross-Validation Accuracy: {cv_scores.mean()}\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Step 5: Evaluation\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    # Convert string labels to integers\n",
        "    y_val_int = y_val.astype(int)\n",
        "    y_pred_int = y_pred.astype(int)\n",
        "\n",
        "    # Calculate precision with pos_label=1\n",
        "    precision = precision_score(y_val_int, y_pred_int, pos_label=1)\n",
        "\n",
        "    # Calculate recall with pos_label=1\n",
        "    recall = recall_score(y_val_int, y_pred_int, pos_label=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    # Manually calculate F1-score\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    print(f\"{name} Evaluation Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-score: {f1}\")\n",
        "\n",
        "# Evaluation\n",
        "best_model = max(models.items(), key=lambda x: cross_val_score(x[1], X_train, y_train, cv=10, scoring='accuracy').mean())[1]\n",
        "y_test_pred = best_model.predict(tfidf_vectorizer.transform(test_data['text']))\n",
        "test_accuracy = accuracy_score(test_data['label'], y_test_pred)\n",
        "\n",
        "# Convert string labels to integers for test data\n",
        "test_data_int = test_data['label'].astype(int)\n",
        "y_test_pred_int = y_test_pred.astype(int)\n",
        "\n",
        "# Calculate precision with pos_label=1 for test data\n",
        "test_precision = precision_score(test_data_int, y_test_pred_int, pos_label=1)\n",
        "\n",
        "# Calculate recall with pos_label=1 for test data\n",
        "test_recall = recall_score(test_data_int, y_test_pred_int, pos_label=1)\n",
        "\n",
        "# Manually alculate F1-score for test data\n",
        "test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall) if (test_precision + test_recall) > 0 else 0\n",
        "\n",
        "print(\"Final Evaluation Metrics on Test Data:\")\n",
        "print(f\"Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1-score: {test_f1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "outputId": "7f2cba33-be22-4d01-817d-b1d0b72666cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                        Product Name Brand Name   Price  \\\n",
            "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "\n",
            "   Rating                                            Reviews  Review Votes  \n",
            "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
            "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
            "2       5                                       Very pleased           0.0  \n",
            "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
            "4       4  Great phone to replace my lost phone. The only...           0.0  \n",
            "0    feel lucky found used phone us used hard phone...\n",
            "1    nice phone nice grade pantach revue clean set ...\n",
            "2                                              pleased\n",
            "3       works good goes slow sometimes good phone love\n",
            "4    great phone replace lost phone thing volume bu...\n",
            "Name: Cleaned_Reviews, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# installing NLTK stopwords\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('punkt')\n",
        "\n",
        "# Loading the dataset\n",
        "dataset_path = \"Amazon_Unlocked_Mobile.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Dropping rows with missing values in the 'Reviews' column\n",
        "df.dropna(subset=['Reviews'], inplace=True)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()  # Convert to string to handle potential NaN values\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Join tokens back into a string\n",
        "    preprocessed_text = ' '.join(filtered_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Apply preprocessing to the 'Reviews' column\n",
        "df['Cleaned_Reviews'] = df['Reviews'].apply(preprocess_text)\n",
        "\n",
        "# Display the preprocessed text\n",
        "print(df['Cleaned_Reviews'].head())\n",
        "\n",
        "# Save the preprocessed data to a new CSV file\n",
        "preprocessed_dataset_path = \"Amazon_Unlocked_Mobile_Preprocessed.csv\"\n",
        "df.to_csv(preprocessed_dataset_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXAlIB2e-a0A",
        "outputId": "526a4112-8d0b-48db-9048-28448c065e23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of TF-IDF matrix: (413770, 1000)\n",
            "Vocabulary (Feature Names): ['10' '100' '12' '15' '16gb' '20' '200' '2g' '30' '32gb']\n"
          ]
        }
      ],
      "source": [
        "#Text Feature Extraction using TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "\n",
        "# Fit and transform the preprocessed text data\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(df['Cleaned_Reviews'])\n",
        "\n",
        "# Display the shape of the TF-IDF matrix\n",
        "print(\"Shape of TF-IDF matrix:\", tfidf_features.shape)\n",
        "\n",
        "# Optional: Display the vocabulary (i.e., feature names)\n",
        "print(\"Vocabulary (Feature Names):\", tfidf_vectorizer.get_feature_names_out()[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hjokX1t-a0A",
        "outputId": "0bb2c5c9-3833-4c8e-c4f6-763a20e184ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "K-means Cluster Labels: [2 2 2 ... 2 2 2]\n"
          ]
        }
      ],
      "source": [
        "#Kmeans\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Initialize K-means with the desired number of clusters\n",
        "num_clusters = 5  # You can adjust the number of clusters as needed\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "\n",
        "# Fit K-means to the TF-IDF features\n",
        "kmeans.fit(tfidf_features)\n",
        "\n",
        "# Get cluster labels\n",
        "kmeans_labels = kmeans.labels_\n",
        "\n",
        "# Display the cluster labels\n",
        "print(\"K-means Cluster Labels:\", kmeans_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHzEAiIH-a0A",
        "outputId": "2819c4b6-8d15-4e18-f461-088a70cf4384"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word:  , Cluster Label: 0\n",
            "Word: e, Cluster Label: 0\n",
            "Word: o, Cluster Label: 0\n",
            "Word: t, Cluster Label: 0\n",
            "Word: n, Cluster Label: 0\n",
            "Word: a, Cluster Label: 0\n",
            "Word: r, Cluster Label: 0\n",
            "Word: s, Cluster Label: 0\n",
            "Word: i, Cluster Label: 0\n",
            "Word: l, Cluster Label: 0\n",
            "Word: d, Cluster Label: 0\n",
            "Word: p, Cluster Label: 0\n",
            "Word: c, Cluster Label: 0\n",
            "Word: h, Cluster Label: 0\n",
            "Word: u, Cluster Label: 0\n",
            "Word: g, Cluster Label: 0\n",
            "Word: m, Cluster Label: 0\n",
            "Word: y, Cluster Label: 0\n",
            "Word: w, Cluster Label: 0\n",
            "Word: b, Cluster Label: 0\n",
            "Word: k, Cluster Label: 0\n",
            "Word: f, Cluster Label: 0\n",
            "Word: v, Cluster Label: 0\n",
            "Word: x, Cluster Label: 0\n",
            "Word: 0, Cluster Label: 1\n",
            "Word: z, Cluster Label: 0\n",
            "Word: 1, Cluster Label: 1\n",
            "Word: 2, Cluster Label: 1\n",
            "Word: q, Cluster Label: 0\n",
            "Word: 5, Cluster Label: 1\n",
            "Word: 3, Cluster Label: 1\n",
            "Word: 4, Cluster Label: 1\n",
            "Word: 6, Cluster Label: 1\n",
            "Word: j, Cluster Label: 0\n",
            "Word: 8, Cluster Label: 1\n",
            "Word: 7, Cluster Label: 1\n",
            "Word: 9, Cluster Label: 1\n",
            "Word: ’, Cluster Label: 4\n",
            "Word: é, Cluster Label: 2\n",
            "Word: ó, Cluster Label: 2\n",
            "Word: í, Cluster Label: 2\n",
            "Word: 👍, Cluster Label: 3\n",
            "Word: á, Cluster Label: 2\n",
            "Word: ñ, Cluster Label: 2\n",
            "Word: ”, Cluster Label: 4\n",
            "Word: 😊, Cluster Label: 3\n",
            "Word: …, Cluster Label: 4\n",
            "Word: •, Cluster Label: 0\n",
            "Word: ú, Cluster Label: 2\n",
            "Word: “, Cluster Label: 4\n",
            "Word: ，, Cluster Label: 0\n",
            "Word: –, Cluster Label: 0\n",
            "Word: 😍, Cluster Label: 3\n",
            "Word: 😡, Cluster Label: 4\n",
            "Word: 👌, Cluster Label: 3\n",
            "Word: ️, Cluster Label: 3\n",
            "Word: ❤, Cluster Label: 3\n",
            "Word: ​, Cluster Label: 3\n",
            "Word: 😀, Cluster Label: 3\n",
            "Word: 😁, Cluster Label: 3\n",
            "Word: 🏻, Cluster Label: 3\n",
            "Word: ¡, Cluster Label: 3\n",
            "Word: █, Cluster Label: 0\n",
            "Word: т, Cluster Label: 3\n",
            "Word: 👎, Cluster Label: 3\n",
            "Word: ☆, Cluster Label: 3\n",
            "Word: ！, Cluster Label: 0\n",
            "Word: ı, Cluster Label: 3\n",
            "Word: ι, Cluster Label: 3\n",
            "Word: 🏽, Cluster Label: 3\n",
            "Word: ☺, Cluster Label: 3\n",
            "Word: 😂, Cluster Label: 3\n",
            "Word: —, Cluster Label: 0\n",
            "Word: н, Cluster Label: 3\n",
            "Word: 💯, Cluster Label: 3\n",
            "Word: 😠, Cluster Label: 3\n",
            "Word: ѕ, Cluster Label: 3\n",
            "Word: 😄, Cluster Label: 0\n",
            "Word: ‘, Cluster Label: 0\n",
            "Word: 📱, Cluster Label: 3\n",
            "Word: ü, Cluster Label: 3\n",
            "Word: 🏾, Cluster Label: 3\n",
            "Word: 😘, Cluster Label: 3\n",
            "Word: 🏼, Cluster Label: 3\n",
            "Word: 😃, Cluster Label: 0\n",
            "Word: 💕, Cluster Label: 3\n",
            "Word: à, Cluster Label: 0\n",
            "Word: 🙌, Cluster Label: 3\n",
            "Word: 😎, Cluster Label: 0\n",
            "Word: ⭐, Cluster Label: 3\n",
            "Word: ã, Cluster Label: 0\n",
            "Word: ա, Cluster Label: 3\n",
            "Word: 😒, Cluster Label: 0\n",
            "Word: ¨, Cluster Label: 0\n",
            "Word: ç, Cluster Label: 0\n",
            "Word: υ, Cluster Label: 3\n",
            "Word: м, Cluster Label: 3\n",
            "Word: в, Cluster Label: 3\n",
            "Word: ®, Cluster Label: 0\n",
            "Word: 😢, Cluster Label: 0\n",
            "Word: ？, Cluster Label: 0\n",
            "Word: 😕, Cluster Label: 0\n",
            "Word: ★, Cluster Label: 0\n",
            "Word: 😭, Cluster Label: 0\n",
            "Word: 😩, Cluster Label: 0\n",
            "Word: є, Cluster Label: 0\n",
            "Word: €, Cluster Label: 0\n",
            "Word: ░, Cluster Label: 0\n",
            "Word: 💋, Cluster Label: 3\n",
            "Word: ♥, Cluster Label: 0\n",
            "Word: 😉, Cluster Label: 0\n",
            "Word: օ, Cluster Label: 0\n",
            "Word: £, Cluster Label: 0\n",
            "Word: °, Cluster Label: 0\n",
            "Word: 。, Cluster Label: 0\n",
            "Word: 😞, Cluster Label: 0\n",
            "Word: è, Cluster Label: 0\n",
            "Word: 😣, Cluster Label: 0\n",
            "Word: ո, Cluster Label: 0\n",
            "Word: ò, Cluster Label: 0\n",
            "Word: 😆, Cluster Label: 0\n",
            "Word: 😤, Cluster Label: 0\n",
            "Word: 😬, Cluster Label: 0\n",
            "Word: 😔, Cluster Label: 0\n",
            "Word: 💪, Cluster Label: 0\n",
            "Word: ♡, Cluster Label: 0\n",
            "Word: 🌟, Cluster Label: 3\n",
            "Word: σ, Cluster Label: 0\n",
            "Word: ì, Cluster Label: 0\n",
            "Word: ғ, Cluster Label: 0\n",
            "Word: 😪, Cluster Label: 0\n",
            "Word: о, Cluster Label: 3\n",
            "Word: ‼, Cluster Label: 0\n",
            "Word: ☎, Cluster Label: 0\n",
            "Word: ½, Cluster Label: 0\n",
            "Word: ¿, Cluster Label: 0\n",
            "Word: 👏, Cluster Label: 0\n",
            "Word: ւ, Cluster Label: 0\n",
            "Word: ռ, Cluster Label: 0\n",
            "Word: ₹, Cluster Label: 0\n",
            "Word: 😅, Cluster Label: 0\n",
            "Word: 🎁, Cluster Label: 0\n",
            "Word: 😈, Cluster Label: 0\n",
            "Word: ✌, Cluster Label: 0\n",
            "Word: ™, Cluster Label: 0\n",
            "Word: ʏ, Cluster Label: 0\n",
            "Word: 🤗, Cluster Label: 0\n",
            "Word: 💖, Cluster Label: 0\n",
            "Word: 📲, Cluster Label: 0\n",
            "Word: 😌, Cluster Label: 0\n",
            "Word: е, Cluster Label: 0\n",
            "Word: º, Cluster Label: 0\n",
            "Word: ♠, Cluster Label: 0\n",
            "Word: ♣, Cluster Label: 0\n",
            "Word: ր, Cluster Label: 0\n",
            "Word: ɨ, Cluster Label: 0\n",
            "Word: ·, Cluster Label: 0\n",
            "Word: ʍ, Cluster Label: 0\n",
            "Word: ɦ, Cluster Label: 0\n",
            "Word: ҡ, Cluster Label: 0\n",
            "Word: ʀ, Cluster Label: 0\n",
            "Word: 😳, Cluster Label: 0\n",
            "Word: ɖ, Cluster Label: 0\n",
            "Word: 📷, Cluster Label: 0\n",
            "Word: ş, Cluster Label: 0\n",
            "Word: 🔥, Cluster Label: 0\n",
            "Word: 💃, Cluster Label: 0\n",
            "Word: ✔, Cluster Label: 0\n",
            "Word: 💞, Cluster Label: 0\n",
            "Word: ê, Cluster Label: 0\n",
            "Word: 💜, Cluster Label: 0\n",
            "Word: ø, Cluster Label: 0\n",
            "Word: ✊, Cluster Label: 0\n",
            "Word: 💙, Cluster Label: 0\n",
            "Word: ℅, Cluster Label: 0\n",
            "Word: ̇, Cluster Label: 0\n",
            "Word: с, Cluster Label: 0\n",
            "Word: 😱, Cluster Label: 0\n",
            "Word: 😖, Cluster Label: 0\n",
            "Word: 💗, Cluster Label: 0\n",
            "Word: ե, Cluster Label: 0\n",
            "Word: 💓, Cluster Label: 0\n",
            "Word: ¸, Cluster Label: 0\n",
            "Word: â, Cluster Label: 0\n",
            "Word: ğ, Cluster Label: 0\n",
            "Word: 🆒, Cluster Label: 0\n",
            "Word: ×, Cluster Label: 0\n",
            "Word: ❖, Cluster Label: 0\n",
            "Word: 😑, Cluster Label: 0\n",
            "Word: յ, Cluster Label: 0\n",
            "Word: 🙏, Cluster Label: 0\n",
            "Word: ♦, Cluster Label: 0\n",
            "Word: ∞, Cluster Label: 0\n",
            "Word: α, Cluster Label: 0\n",
            "Word: ч, Cluster Label: 0\n",
            "Word: 📶, Cluster Label: 0\n",
            "Word: մ, Cluster Label: 0\n",
            "Word: ±, Cluster Label: 0\n",
            "Word: 📞, Cluster Label: 0\n",
            "Word: ）, Cluster Label: 0\n",
            "Word: ù, Cluster Label: 0\n",
            "Word: 发, Cluster Label: 0\n",
            "Word: 💰, Cluster Label: 0\n",
            "Word: 💚, Cluster Label: 0\n",
            "Word: ☮, Cluster Label: 0\n",
            "Word: 🔋, Cluster Label: 0\n",
            "Word: վ, Cluster Label: 0\n",
            "Word: կ, Cluster Label: 0\n",
            "Word: а, Cluster Label: 0\n",
            "Word: 😏, Cluster Label: 0\n",
            "Word: հ, Cluster Label: 0\n",
            "Word: 🙄, Cluster Label: 0\n",
            "Word: ï, Cluster Label: 0\n",
            "Word: 🖒, Cluster Label: 0\n",
            "Word: ¤, Cluster Label: 0\n",
            "Word: ý, Cluster Label: 0\n",
            "Word: ĸ, Cluster Label: 0\n",
            "Word: 💘, Cluster Label: 0\n",
            "Word: 🙋, Cluster Label: 0\n",
            "Word: 🎆, Cluster Label: 0\n",
            "Word: 🔛, Cluster Label: 0\n",
            "Word: 💟, Cluster Label: 0\n",
            "Word: , Cluster Label: 0\n",
            "Word: ☯, Cluster Label: 0\n",
            "Word: 🅰, Cluster Label: 0\n",
            "Word: ³, Cluster Label: 0\n",
            "Word: （, Cluster Label: 0\n",
            "Word: 😦, Cluster Label: 0\n",
            "Word: л, Cluster Label: 0\n",
            "Word: 😫, Cluster Label: 0\n",
            "Word: д, Cluster Label: 0\n",
            "Word: ś, Cluster Label: 0\n",
            "Word: к, Cluster Label: 0\n",
            "Word: 不, Cluster Label: 0\n",
            "Word: 😥, Cluster Label: 0\n",
            "Word: 😟, Cluster Label: 0\n",
            "Word: 👆, Cluster Label: 0\n",
            "Word: 😜, Cluster Label: 0\n",
            "Word: 🙂, Cluster Label: 0\n",
            "Word: 국, Cluster Label: 0\n",
            "Word: 🤔, Cluster Label: 0\n",
            "Word: ö, Cluster Label: 0\n",
            "Word: 😋, Cluster Label: 0\n",
            "Word: ¯, Cluster Label: 0\n",
            "Word: 好, Cluster Label: 0\n",
            "Word: µ, Cluster Label: 0\n",
            "Word: 한, Cluster Label: 0\n",
            "Word: 에, Cluster Label: 0\n",
            "Word: 🏿, Cluster Label: 0\n",
            "Word: ä, Cluster Label: 0\n",
            "Word: ը, Cluster Label: 0\n",
            "Word: 😚, Cluster Label: 0\n",
            "Word: 서, Cluster Label: 0\n",
            "Word: 😮, Cluster Label: 0\n",
            "Word: 니, Cluster Label: 0\n",
            "Word: 다, Cluster Label: 0\n",
            "Word: ս, Cluster Label: 0\n",
            "Word: 😿, Cluster Label: 0\n",
            "Word: 😹, Cluster Label: 0\n",
            "Word: ն, Cluster Label: 0\n",
            "Word: 😐, Cluster Label: 0\n",
            "Word: ،, Cluster Label: 0\n",
            "Word: 🔝, Cluster Label: 0\n",
            "Word: ҽ, Cluster Label: 0\n",
            "Word: 👊, Cluster Label: 0\n",
            "Word: ɷ, Cluster Label: 0\n",
            "Word: 🌈, Cluster Label: 0\n",
            "Word: ¬, Cluster Label: 0\n",
            "Word: 맞, Cluster Label: 0\n",
            "Word: š, Cluster Label: 0\n",
            "Word: 🔫, Cluster Label: 0\n",
            "Word: ¢, Cluster Label: 0\n",
            "Word: 🎶, Cluster Label: 0\n",
            "Word: 잘, Cluster Label: 0\n",
            "Word: 됩, Cluster Label: 0\n",
            "Word: õ, Cluster Label: 0\n",
            "Word: 언, Cluster Label: 0\n",
            "Word: 락, Cluster Label: 0\n",
            "Word: 음, Cluster Label: 0\n",
            "Word: 的, Cluster Label: 0\n",
            "Word: 🙁, Cluster Label: 0\n",
            "Word: ¥, Cluster Label: 0\n",
            "Word: ○, Cluster Label: 0\n",
            "Word: 😵, Cluster Label: 0\n",
            "Word: ы, Cluster Label: 0\n",
            "Word: р, Cluster Label: 0\n",
            "Word: ш, Cluster Label: 0\n",
            "Word: ɧ, Cluster Label: 0\n",
            "Word: 😝, Cluster Label: 0\n",
            "Word: 版, Cluster Label: 0\n",
            "Word: ≡, Cluster Label: 0\n",
            "Word: 🎉, Cluster Label: 0\n",
            "Word: и, Cluster Label: 0\n",
            "Word: й, Cluster Label: 0\n",
            "Word: ©, Cluster Label: 0\n",
            "Word: ⚠, Cluster Label: 0\n",
            "Word: я, Cluster Label: 0\n",
            "Word: ­, Cluster Label: 0\n",
            "Word: 🚮, Cluster Label: 0\n",
            "Word: →, Cluster Label: 0\n",
            "Word: 😧, Cluster Label: 0\n",
            "Word: ь, Cluster Label: 0\n",
            "Word: 卡, Cluster Label: 0\n",
            "Word: ŋ, Cluster Label: 0\n",
            "Word: ‎, Cluster Label: 0\n",
            "Word: 💛, Cluster Label: 0\n",
            "Word: ≤, Cluster Label: 0\n",
            "Word: 🤘, Cluster Label: 0\n",
            "Word: 😛, Cluster Label: 0\n",
            "Word: 💳, Cluster Label: 0\n",
            "Word: 大, Cluster Label: 0\n",
            "Word: 💔, Cluster Label: 0\n",
            "Word: 看, Cluster Label: 0\n",
            "Word: ：, Cluster Label: 0\n",
            "Word: 🔓, Cluster Label: 0\n",
            "Word: , Cluster Label: 0\n",
            "Word: 索, Cluster Label: 0\n",
            "Word: 法, Cluster Label: 0\n",
            "Word: 😙, Cluster Label: 0\n",
            "Word: 尼, Cluster Label: 0\n",
            "Word: ☀, Cluster Label: 0\n",
            "Word: դ, Cluster Label: 0\n",
            "Word: շ, Cluster Label: 0\n",
            "Word: 😰, Cluster Label: 0\n",
            "Word: 是, Cluster Label: 0\n",
            "Word: բ, Cluster Label: 0\n",
            "Word: 哥, Cluster Label: 0\n",
            "Word: թ, Cluster Label: 0\n",
            "Word: ք, Cluster Label: 0\n",
            "Word: խ, Cluster Label: 0\n",
            "Word: ։, Cluster Label: 0\n",
            "Word: ➕, Cluster Label: 0\n",
            "Word: ‏, Cluster Label: 0\n",
            "Word: 🚚, Cluster Label: 0\n",
            "Word: ﻿, Cluster Label: 0\n",
            "Word: ⊙, Cluster Label: 0\n",
            "Word: г, Cluster Label: 0\n",
            "Word: ā, Cluster Label: 0\n",
            "Word: э, Cluster Label: 0\n",
            "Word: 微, Cluster Label: 0\n",
            "Word: 用, Cluster Label: 0\n",
            "Word: 父, Cluster Label: 0\n",
            "Word: 国, Cluster Label: 0\n",
            "Word: 笑, Cluster Label: 0\n",
            "Word: 姨, Cluster Label: 0\n",
            "Word: 👋, Cluster Label: 0\n",
            "Word: ų, Cluster Label: 0\n",
            "Word: 😇, Cluster Label: 0\n",
            "Word: 应, Cluster Label: 0\n",
            "Word: у, Cluster Label: 0\n",
            "Word: 反, Cluster Label: 0\n",
            "Word: ア, Cluster Label: 0\n",
            "Word: オ, Cluster Label: 0\n",
            "Word: ✓, Cluster Label: 0\n",
            "Word: ❕, Cluster Label: 0\n",
            "Word: 😨, Cluster Label: 0\n",
            "Word: ¾, Cluster Label: 0\n",
            "Word: 常, Cluster Label: 0\n",
            "Word: 非, Cluster Label: 0\n",
            "Word: 😓, Cluster Label: 0\n",
            "Word: 하, Cluster Label: 0\n",
            "Word: ى, Cluster Label: 0\n",
            "Word: ✈, Cluster Label: 0\n",
            "Word: 👽, Cluster Label: 0\n",
            "Word: 💁, Cluster Label: 0\n",
            "Word: 이, Cluster Label: 0\n",
            "Word: 였, Cluster Label: 0\n",
            "Word: 💸, Cluster Label: 0\n",
            "Word: 🍎, Cluster Label: 0\n",
            "Word: , Cluster Label: 0\n",
            "Word: 🚫, Cluster Label: 0\n",
            "Word: ‰, Cluster Label: 0\n",
            "Word: ✨, Cluster Label: 0\n",
            "Word: 🍭, Cluster Label: 0\n",
            "Word: 👮, Cluster Label: 0\n",
            "Word: 🙆, Cluster Label: 0\n",
            "Word: 💻, Cluster Label: 0\n",
            "Word: 🏢, Cluster Label: 0\n",
            "Word: 👦, Cluster Label: 0\n",
            "Word: ɩ, Cluster Label: 0\n",
            "Word: ☹, Cluster Label: 0\n",
            "Word: 😯, Cluster Label: 0\n",
            "Word: 就, Cluster Label: 0\n",
            "Word: 🎮, Cluster Label: 0\n",
            "Word: п, Cluster Label: 0\n",
            "Word: 💡, Cluster Label: 0\n",
            "Word: √, Cluster Label: 0\n",
            "Word: ф, Cluster Label: 0\n",
            "Word: 语, Cluster Label: 0\n",
            "Word: 言, Cluster Label: 0\n",
            "Word: 🙅, Cluster Label: 0\n",
            "Word: 🐌, Cluster Label: 0\n",
            "Word: 还, Cluster Label: 0\n",
            "Word: з, Cluster Label: 0\n",
            "Word: ａ, Cluster Label: 0\n",
            "Word: ｉ, Cluster Label: 0\n",
            "Word: ｋ, Cluster Label: 0\n",
            "Word: ｏ, Cluster Label: 0\n",
            "Word: ｎ, Cluster Label: 0\n",
            "Word: 👯, Cluster Label: 0\n",
            "Word: 🐖, Cluster Label: 0\n",
            "Word: б, Cluster Label: 0\n",
            "Word: ☝, Cluster Label: 0\n",
            "Word: ū, Cluster Label: 0\n",
            "Word: 습, Cluster Label: 0\n",
            "Word: §, Cluster Label: 0\n",
            "Word: æ, Cluster Label: 0\n",
            "Word: ě, Cluster Label: 0\n",
            "Word: 💵, Cluster Label: 0\n",
            "Word: , Cluster Label: 0\n",
            "Word: 요, Cluster Label: 0\n",
            "Word: 어, Cluster Label: 0\n",
            "Word: 품, Cluster Label: 0\n",
            "Word: 양, Cluster Label: 0\n",
            "Word: ẃ, Cluster Label: 0\n",
            "Word: 👑, Cluster Label: 0\n",
            "Word: ж, Cluster Label: 0\n",
            "Word: ŕ, Cluster Label: 0\n",
            "Word: 🔊, Cluster Label: 0\n",
            "Word: 💦, Cluster Label: 0\n",
            "Word: », Cluster Label: 0\n",
            "Word: ✋, Cluster Label: 0\n",
            "Word: 군, Cluster Label: 0\n",
            "Word: 좋, Cluster Label: 0\n",
            "Word: 》, Cluster Label: 0\n",
            "Word: , Cluster Label: 0\n",
            "Word: 够, Cluster Label: 0\n",
            "Word: 度, Cluster Label: 0\n",
            "Word: 速, Cluster Label: 0\n",
            "Word: 圾, Cluster Label: 0\n",
            "Word: 垃, Cluster Label: 0\n",
            "Word: 较, Cluster Label: 0\n",
            "Word: 比, Cluster Label: 0\n",
            "Word: 器, Cluster Label: 0\n",
            "Word: 理, Cluster Label: 0\n",
            "Word: 处, Cluster Label: 0\n",
            "Word: 🤓, Cluster Label: 0\n",
            "Word: 고, Cluster Label: 0\n",
            "Word: 매, Cluster Label: 0\n",
            "Word: 错, Cluster Label: 0\n",
            "Word: 来, Cluster Label: 0\n",
            "Word: х, Cluster Label: 0\n",
            "Word: , Cluster Label: 0\n",
            "Word: 🎤, Cluster Label: 0\n",
            "Word: 😶, Cluster Label: 0\n",
            "Word: 了, Cluster Label: 0\n",
            "Word: 偷, Cluster Label: 0\n",
            "Word: 被, Cluster Label: 0\n",
            "Word: 天, Cluster Label: 0\n",
            "Word: 三, Cluster Label: 0\n",
            "Word: 买, Cluster Label: 0\n",
            "Word: 구, Cluster Label: 0\n",
            "Word: 动, Cluster Label: 0\n",
            "Word: 移, Cluster Label: 0\n",
            "Word: 通, Cluster Label: 0\n",
            "Word: 联, Cluster Label: 0\n",
            "Word: 持, Cluster Label: 0\n",
            "Word: 支, Cluster Label: 0\n",
            "Word: 贴, Cluster Label: 0\n",
            "Word: 要, Cluster Label: 0\n",
            "Word: 中, Cluster Label: 0\n",
            "Word: 赞, Cluster Label: 0\n",
            "Word: 很, Cluster Label: 0\n",
            "Word: 喜, Cluster Label: 0\n",
            "Word: 欢, Cluster Label: 0\n",
            "Word: 해, Cluster Label: 0\n",
            "Word: 위, Cluster Label: 0\n",
            "Word: 기, Cluster Label: 0\n",
            "Word: 용, Cluster Label: 0\n",
            "Word: 사, Cluster Label: 0\n",
            "Word: 美, Cluster Label: 0\n",
            "Word: 欧, Cluster Label: 0\n",
            "Word: 这, Cluster Label: 0\n",
            "Word: 家, Cluster Label: 0\n",
            "Word: 个, Cluster Label: 0\n",
            "Word: 那, Cluster Label: 0\n",
            "Word: 道, Cluster Label: 0\n",
            "Word: 知, Cluster Label: 0\n",
            "Word: 号, Cluster Label: 0\n",
            "Word: 符, Cluster Label: 0\n",
            "Word: 上, Cluster Label: 0\n",
            "Word: 盘, Cluster Label: 0\n",
            "Word: 键, Cluster Label: 0\n",
            "Word: 快, Cluster Label: 0\n",
            "Word: 也, Cluster Label: 0\n"
          ]
        }
      ],
      "source": [
        "#Word2vec\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Train Word2Vec model (or load pre-trained model)\n",
        "word2vec_model = Word2Vec(sentences=df['Cleaned_Reviews'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Get word embeddings\n",
        "word_embeddings = word2vec_model.wv\n",
        "\n",
        "# Get word vectors\n",
        "word_vectors = word_embeddings.vectors\n",
        "\n",
        "# Apply K-means clustering to word vectors\n",
        "num_clusters = 5  # You can adjust the number of clusters as needed\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "word2vec_cluster_labels = kmeans.fit_predict(word_vectors)\n",
        "\n",
        "# Output cluster labels for the first few words\n",
        "for word, cluster_label in zip(word_embeddings.index_to_key, word2vec_cluster_labels):\n",
        "    print(f\"Word: {word}, Cluster Label: {cluster_label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj9aUfxR-a0B"
      },
      "outputs": [],
      "source": [
        "#DBSCAN\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Initialize DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "\n",
        "# Fit DBSCAN to the TF-IDF features\n",
        "dbscan.fit(tfidf_features)\n",
        "\n",
        "# Get cluster labels (Note: DBSCAN assigns -1 to noise points)\n",
        "dbscan_labels = dbscan.labels_\n",
        "\n",
        "# Display the cluster labels\n",
        "print(\"DBSCAN Cluster Labels:\", dbscan_labels[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRijW2aLGONl"
      },
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIYCj5qyGfSL"
      },
      "source": [
        "**Write your response here:**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEs-OoDEhTW4"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUKC7suYhVl0"
      },
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "In this exercise, I successively implemented MultinomialNB, SVM, KNN, Decision Tree, Random Forest, XGBoost algorithms for sentiment analysis.\n",
        "I used 10-fold cross-validation to ensure robust model evaluation and reduce overfitting. Various metrics like Accuracy, Recall, Precision, and F1-score to comprehensively\n",
        "assess model performance. In exercise 2, I utilized the Amazon mobile phone reviews dataset for clustering. I then finally\n",
        "implemented K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT for text clustering tasks.\n",
        "Challenges:\n",
        "The Amazon mobile phone reviews dataset was too large so I experienced memory allocation error.\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}